{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa16c34-11a2-423f-b6f3-ebf956daac2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./encryption_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54973760-1014-4d64-80ad-db4bf33fcaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, to_timestamp, to_date, when\n",
    "from pyspark.sql.types import IntegerType, LongType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "encryptor = PIIEncryptor() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d2e071-e74c-4fb6-8fee-eee1a0808760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def decrypt_clean_encrypt(df, pii_fields, critical_fields, dedup_cols):\n",
    "    # decrypt df\n",
    "    df = encryptor.decrypt_dataframe(df, pii_fields)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        # trim whitespace\n",
    "        df = df.withColumn(c, trim(col(c)))\n",
    "        # replace blanks with null\n",
    "        df = df.withColumn(c, when(col(c) == \"\", None).otherwise(col(c)))\n",
    "        # cast IDs to integer/long if column name contains 'id'\n",
    "        if \"id\" in c.lower():\n",
    "            df = df.withColumn(c, col(c).cast(LongType()))\n",
    "        # rename columns to lowercase\n",
    "        df = df.withColumnRenamed(c, c.lower())\n",
    "\n",
    "    # standardize date/timestamp columns\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c.lower():\n",
    "            df = df.withColumn(c, to_date(col(c)))\n",
    "        if \"time\" in c.lower():\n",
    "            df = df.withColumn(c, to_timestamp(col(c)))\n",
    "\n",
    "    # remove records with nulls in critical fields\n",
    "    if critical_fields:\n",
    "        df = df.dropna(subset=critical_fields)\n",
    "\n",
    "    # remove duplicates (streaming-safe)\n",
    "    if dedup_cols:\n",
    "        df = df \\\n",
    "            .withWatermark(\"ingest_time\", \"2 hours\") \\\n",
    "            .dropDuplicates(dedup_cols)\n",
    "\n",
    "    # encrypt PII columns again\n",
    "    df = encryptor.encrypt_dataframe(df, pii_fields)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a098c9c-ed66-4bbd-a8e8-fc815ce78a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f30360322a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_weather_pii_columns = [\n",
    "    \"address\",\n",
    "    \"name\"\n",
    "]\n",
    "hotel_weather_critical_fields = [\"id\", \"wthr_date\"]\n",
    "hotel_weather_dedupe_cols = [\"id\", \"wthr_date\"]\n",
    "\n",
    "# read streaming bronze\n",
    "hotel_bronze = (\n",
    "    spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(\"bronze.hotel_weather_raw\")\n",
    "        .withColumn(\"ingest_time\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "hotel_clean = decrypt_clean_encrypt(\n",
    "    hotel_bronze,\n",
    "    pii_fields=hotel_weather_pii_columns,\n",
    "    critical_fields=hotel_weather_critical_fields,\n",
    "    dedup_cols=hotel_weather_dedupe_cols\n",
    ")\n",
    "\n",
    "# write streaming silver\n",
    "(\n",
    "    hotel_clean.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/mnt/checkpoints/silver_hotel_weather_n\")\n",
    "        .table(\"silver.hotel_weather_processed\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5996699-f735-49fe-ae2c-46c68adff50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}